{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "SPLITS VALIDATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "names = [\"FB15k\", \"FB15k237\", \"WN18\", \"WN18RR\", \"YAGO310\"]\n",
    "name_splits = [\"Split_0\", \"Split_1\", \"Split_2\", \"Split_3\", \"Split_4\"]\n",
    "splitted_data = [\"training\", \"testing\" ,\"validation\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def check_quality(df):\n",
    "                #analyze correlations\n",
    "                c = df.corr(method ='pearson').abs()\n",
    "                s = c.unstack()\n",
    "                s = s[s>0.5]\n",
    "                s = s[s<1]\n",
    "                for i in range(0,len(s)):\n",
    "                    print(str(s.axes[0][i]) + \" have high correlation\")\n",
    "\n",
    "                #analyze missing values\n",
    "                if (df.isnull().sum().sum() != 0):\n",
    "                    print(name + \"_\" + name_split + \"_\" + split_data + \" have \" + str(df.isnull().sum().sum()) + \" missing values\")\n",
    "\n",
    "                #check for duplicates\n",
    "                if (df.duplicated().any()):\n",
    "                    print(name + \"_\" + name_split + \"_\" + split_data + \" have \" + \" duplicated rows\")\n",
    "\n",
    "def check_split(training, testing, validation):\n",
    "\n",
    "    def check(df1,df2):\n",
    "        check = pd.merge(df1, df2, on=['h', 'r', 't'], how='left', indicator='Exist')\n",
    "        check_both = check[check['Exist'] == 'both']\n",
    "        if (df1.duplicated().any()):\n",
    "            print(name + \"_\" + name_split + \"_\" + split_data + \" and \" + split_data_0 + \" are overlapping\")\n",
    "\n",
    "    check(training,testing)\n",
    "    check(training,validation)\n",
    "    check(testing,validation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open FB15k..\n",
      "Open FB15k_Split_0..\n",
      "Open FB15k_Split_0_training..\n",
      "Open FB15k_Split_0_testing..\n",
      "Open FB15k_Split_0_validation..\n",
      "Open FB15k_Split_1..\n",
      "Open FB15k_Split_1_training..\n",
      "Open FB15k_Split_1_testing..\n",
      "Open FB15k_Split_1_validation..\n",
      "Open FB15k_Split_2..\n",
      "Open FB15k_Split_2_training..\n",
      "Open FB15k_Split_2_testing..\n",
      "Open FB15k_Split_2_validation..\n",
      "Open FB15k_Split_3..\n",
      "Open FB15k_Split_3_training..\n",
      "Open FB15k_Split_3_testing..\n",
      "Open FB15k_Split_3_validation..\n",
      "Open FB15k_Split_4..\n",
      "Open FB15k_Split_4_training..\n",
      "Open FB15k_Split_4_testing..\n",
      "Open FB15k_Split_4_validation..\n",
      "FB15k_training overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     80.1  |     80.17 |     80.1  |     80.14 |\n",
      "| Split_1 |     80.1  |    100    |     80.14 |     80.18 |     80.14 |\n",
      "| Split_2 |     80.17 |     80.14 |    100    |     80.13 |     80.16 |\n",
      "| Split_3 |     80.1  |     80.17 |     80.13 |    100    |     80.12 |\n",
      "| Split_4 |     80.14 |     80.14 |     80.16 |     80.12 |    100    |\n",
      "FB15k_testing overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     10.15 |     10.4  |     10.04 |     10.41 |\n",
      "| Split_1 |     10.15 |    100    |     10.02 |     10.5  |     10.43 |\n",
      "| Split_2 |     10.4  |     10.02 |    100    |     10.19 |     10.34 |\n",
      "| Split_3 |     10.04 |     10.5  |     10.19 |    100    |     10.16 |\n",
      "| Split_4 |     10.41 |     10.43 |     10.34 |     10.16 |    100    |\n",
      "FB15k_validation overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     10.17 |     10.27 |     10.3  |     10.23 |\n",
      "| Split_1 |     10.17 |    100    |     10.37 |     10.08 |      9.99 |\n",
      "| Split_2 |     10.27 |     10.37 |    100    |     10.19 |     10.39 |\n",
      "| Split_3 |     10.3  |     10.08 |     10.19 |    100    |     10.13 |\n",
      "| Split_4 |     10.23 |      9.99 |     10.39 |     10.13 |    100    |\n",
      "Open FB15k237..\n",
      "Open FB15k237_Split_0..\n",
      "Open FB15k237_Split_0_training..\n",
      "Open FB15k237_Split_0_testing..\n",
      "Open FB15k237_Split_0_validation..\n",
      "Open FB15k237_Split_1..\n",
      "Open FB15k237_Split_1_training..\n",
      "Open FB15k237_Split_1_testing..\n",
      "Open FB15k237_Split_1_validation..\n",
      "Open FB15k237_Split_2..\n",
      "Open FB15k237_Split_2_training..\n",
      "Open FB15k237_Split_2_testing..\n",
      "Open FB15k237_Split_2_validation..\n",
      "Open FB15k237_Split_3..\n",
      "Open FB15k237_Split_3_training..\n",
      "Open FB15k237_Split_3_testing..\n",
      "Open FB15k237_Split_3_validation..\n",
      "Open FB15k237_Split_4..\n",
      "Open FB15k237_Split_4_training..\n",
      "Open FB15k237_Split_4_testing..\n",
      "Open FB15k237_Split_4_validation..\n",
      "FB15k237_training overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     80.26 |     80.28 |     80.2  |     80.19 |\n",
      "| Split_1 |     80.26 |    100    |     80.22 |     80.26 |     80.22 |\n",
      "| Split_2 |     80.28 |     80.22 |    100    |     80.25 |     80.21 |\n",
      "| Split_3 |     80.2  |     80.26 |     80.25 |    100    |     80.24 |\n",
      "| Split_4 |     80.19 |     80.22 |     80.2  |     80.24 |    100    |\n",
      "FB15k237_testing overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     10.32 |     10.71 |     10.55 |     10.55 |\n",
      "| Split_1 |     10.32 |    100    |     10.46 |     10.66 |     10.24 |\n",
      "| Split_2 |     10.71 |     10.46 |    100    |     10.47 |     10.41 |\n",
      "| Split_3 |     10.55 |     10.66 |     10.47 |    100    |     10.27 |\n",
      "| Split_4 |     10.55 |     10.24 |     10.41 |     10.27 |    100    |\n",
      "FB15k237_validation overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     10.56 |     10.71 |     10.35 |     10.67 |\n",
      "| Split_1 |     10.56 |    100    |     10.68 |     10.27 |     10.45 |\n",
      "| Split_2 |     10.71 |     10.68 |    100    |     10.27 |     10.49 |\n",
      "| Split_3 |     10.35 |     10.27 |     10.27 |    100    |     10.65 |\n",
      "| Split_4 |     10.67 |     10.45 |     10.49 |     10.65 |    100    |\n",
      "Open WN18..\n",
      "Open WN18_Split_0..\n",
      "Open WN18_Split_0_training..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_0_testing..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_0_validation..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_1..\n",
      "Open WN18_Split_1_training..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_1_testing..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_1_validation..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_2..\n",
      "Open WN18_Split_2_training..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_2_testing..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_2_validation..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_3..\n",
      "Open WN18_Split_3_training..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_3_testing..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_3_validation..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_4..\n",
      "Open WN18_Split_4_training..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_4_testing..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18_Split_4_validation..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "WN18_training overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     81.81 |     81.8  |     81.9  |     81.77 |\n",
      "| Split_1 |     81.81 |    100    |     81.91 |     81.79 |     81.84 |\n",
      "| Split_2 |     81.8  |     81.91 |    100    |     81.83 |     81.8  |\n",
      "| Split_3 |     81.9  |     81.79 |     81.84 |    100    |     81.81 |\n",
      "| Split_4 |     81.76 |     81.84 |     81.8  |     81.82 |    100    |\n",
      "WN18_testing overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     13.63 |     13.65 |     13.91 |     13.36 |\n",
      "| Split_1 |     13.63 |    100    |     13.95 |     13.58 |     13.58 |\n",
      "| Split_2 |     13.65 |     13.95 |    100    |     13.4  |     13.44 |\n",
      "| Split_3 |     13.91 |     13.58 |     13.4  |    100    |     13.56 |\n",
      "| Split_4 |     13.36 |     13.58 |     13.44 |     13.56 |    100    |\n",
      "WN18_validation overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     13.89 |     13.09 |     13.85 |     13.94 |\n",
      "| Split_1 |     13.89 |    100    |     13.93 |     13.31 |     13.67 |\n",
      "| Split_2 |     13.09 |     13.93 |    100    |     13.67 |     13.6  |\n",
      "| Split_3 |     13.85 |     13.31 |     13.67 |    100    |     13.8  |\n",
      "| Split_4 |     13.94 |     13.67 |     13.6  |     13.8  |    100    |\n",
      "Open WN18RR..\n",
      "Open WN18RR_Split_0..\n",
      "Open WN18RR_Split_0_training..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_0_testing..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_0_validation..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_1..\n",
      "Open WN18RR_Split_1_training..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_1_testing..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_1_validation..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_2..\n",
      "Open WN18RR_Split_2_training..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_2_testing..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_2_validation..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_3..\n",
      "Open WN18RR_Split_3_training..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_3_testing..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_3_validation..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_4..\n",
      "Open WN18RR_Split_4_training..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_4_testing..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_Split_4_validation..\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "WN18RR_training overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     83.82 |     83.89 |     83.92 |     83.86 |\n",
      "| Split_1 |     83.82 |    100    |     83.76 |     83.91 |     83.9  |\n",
      "| Split_2 |     83.88 |     83.76 |    100    |     83.82 |     83.86 |\n",
      "| Split_3 |     83.93 |     83.9  |     83.83 |    100    |     83.95 |\n",
      "| Split_4 |     83.85 |     83.9  |     83.85 |     83.95 |    100    |\n",
      "WN18RR_testing overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     17.49 |     17.71 |     18.07 |     17.62 |\n",
      "| Split_1 |     17.49 |    100    |     17.16 |     18.07 |     17.79 |\n",
      "| Split_2 |     17.71 |     17.16 |    100    |     17.05 |     17.43 |\n",
      "| Split_3 |     18.07 |     18.07 |     17.05 |    100    |     18.03 |\n",
      "| Split_4 |     17.62 |     17.79 |     17.43 |     18.03 |    100    |\n",
      "WN18RR_validation overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     17.73 |     16.92 |     17.84 |     17.71 |\n",
      "| Split_1 |     17.73 |    100    |     17.52 |     18    |     17.74 |\n",
      "| Split_2 |     16.92 |     17.52 |    100    |     17.8  |     18.11 |\n",
      "| Split_3 |     17.84 |     18    |     17.8  |    100    |     17.65 |\n",
      "| Split_4 |     17.71 |     17.74 |     18.11 |     17.65 |    100    |\n",
      "Open YAGO310..\n",
      "Open YAGO310_Split_0..\n",
      "Open YAGO310_Split_0_training..\n",
      "Open YAGO310_Split_0_testing..\n",
      "Open YAGO310_Split_0_validation..\n",
      "Open YAGO310_Split_1..\n",
      "Open YAGO310_Split_1_training..\n",
      "Open YAGO310_Split_1_testing..\n",
      "Open YAGO310_Split_1_validation..\n",
      "Open YAGO310_Split_2..\n",
      "Open YAGO310_Split_2_training..\n",
      "Open YAGO310_Split_2_testing..\n",
      "Open YAGO310_Split_2_validation..\n",
      "Open YAGO310_Split_3..\n",
      "Open YAGO310_Split_3_training..\n",
      "Open YAGO310_Split_3_testing..\n",
      "Open YAGO310_Split_3_validation..\n",
      "Open YAGO310_Split_4..\n",
      "Open YAGO310_Split_4_training..\n",
      "Open YAGO310_Split_4_testing..\n",
      "Open YAGO310_Split_4_validation..\n",
      "YAGO310_training overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     80.63 |     80.62 |     80.62 |     80.61 |\n",
      "| Split_1 |     80.63 |    100    |     80.63 |     80.63 |     80.62 |\n",
      "| Split_2 |     80.62 |     80.63 |    100    |     80.63 |     80.63 |\n",
      "| Split_3 |     80.62 |     80.63 |     80.63 |    100    |     80.63 |\n",
      "| Split_4 |     80.61 |     80.62 |     80.62 |     80.63 |    100    |\n",
      "YAGO310_testing overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     11.3  |     11.25 |     11.27 |     11.17 |\n",
      "| Split_1 |     11.3  |    100    |     11.31 |     11.28 |     11.2  |\n",
      "| Split_2 |     11.25 |     11.31 |    100    |     11.33 |     11.23 |\n",
      "| Split_3 |     11.27 |     11.28 |     11.33 |    100    |     11.36 |\n",
      "| Split_4 |     11.17 |     11.2  |     11.23 |     11.36 |    100    |\n",
      "YAGO310_validation overlapping matrix:\n",
      "|         |   Split_0 |   Split_1 |   Split_2 |   Split_3 |   Split_4 |\n",
      "|:--------|----------:|----------:|----------:|----------:|----------:|\n",
      "| Split_0 |    100    |     11.14 |     11.25 |     11.28 |     11.2  |\n",
      "| Split_1 |     11.14 |    100    |     11.24 |     11.4  |     11.15 |\n",
      "| Split_2 |     11.25 |     11.24 |    100    |     11.44 |     11.27 |\n",
      "| Split_3 |     11.28 |     11.4  |     11.44 |    100    |     11.3  |\n",
      "| Split_4 |     11.2  |     11.15 |     11.27 |     11.3  |    100    |\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "\n",
    "    print(\"Open \"+name+\"..\")\n",
    "\n",
    "    splits = {\n",
    "    \"Split_0\": pd.DataFrame([]),\n",
    "    \"Split_1\": pd.DataFrame([]),\n",
    "    \"Split_2\": pd.DataFrame([]),\n",
    "    \"Split_3\": pd.DataFrame([]),\n",
    "    \"Split_4\": pd.DataFrame([])\n",
    "    }\n",
    "\n",
    "    for name_split in name_splits:\n",
    "        kg_path = '../Data_Collection/Datasets_Complete/'+name+'/'+name_split+'/instance.pickle'\n",
    "\n",
    "        print(\"Open \"+name + \"_\" + name_split+\"..\")\n",
    "\n",
    "        with open(kg_path, 'rb') as f:\n",
    "            pick = pickle.load(f)\n",
    "\n",
    "            dataset = pick['dataset']\n",
    "\n",
    "            data = {\n",
    "                'dataset': dataset,\n",
    "                'training': pd.DataFrame(dataset['training'], columns=['h', 'r', 't']),\n",
    "                'testing': pd.DataFrame(dataset['testing'], columns=['h', 'r', 't']),\n",
    "                'validation': pd.DataFrame(dataset['validation'], columns=['h', 'r', 't']),\n",
    "            }\n",
    "\n",
    "            splits[name_split] = data\n",
    "\n",
    "            #analyze single split\n",
    "            for split_data in splitted_data:\n",
    "\n",
    "                print(\"Open \"+name + \"_\" + name_split + \"_\" + split_data + \"..\")\n",
    "\n",
    "                df = pd.DataFrame(splits[name_split][split_data])\n",
    "\n",
    "                #analyze correlations\n",
    "                c = df.corr(method ='pearson').abs()\n",
    "                s = c.unstack()\n",
    "                s = s[s>0.5]\n",
    "                s = s[s<1]\n",
    "                for i in range(0,len(s)):\n",
    "                    print(str(s.axes[0][i]) + \" have high correlation\")\n",
    "\n",
    "                df = pd.DataFrame(splits[name_split][split_data])\n",
    "\n",
    "                #analyze missing values\n",
    "                if (df.isnull().sum().sum() != 0):\n",
    "                    print(name + \"_\" + name_split + \"_\" + split_data + \" have \" + str(df.isnull().sum().sum()) + \" missing values\")\n",
    "\n",
    "                #check for duplicates\n",
    "                if (df.duplicated().any()):\n",
    "                    print(name + \"_\" + name_split + \"_\" + split_data + \" have \" + \" duplicated rows\")\n",
    "\n",
    "                #check if the splitting is ok and not overlapping\n",
    "                for split_data_0 in splitted_data:\n",
    "                    df1 = df\n",
    "                    df2 = splits[name_split][split_data_0]\n",
    "                    check = pd.merge(df1, df2, on=['h', 'r', 't'], how='left', indicator='Exist')\n",
    "                    check_both = check[check['Exist'] == 'both']\n",
    "                    if (df1.duplicated().any()):\n",
    "                        print(name + \"_\" + name_split + \"_\" + split_data + \" and \" + split_data_0 + \" are overlapping\")\n",
    "\n",
    "    #percentage of overlapping for different splits\n",
    "    for split_data in splitted_data:\n",
    "\n",
    "        overlapping = pd.DataFrame([],columns=name_splits,index=name_splits)\n",
    "\n",
    "        for name_split in name_splits:\n",
    "\n",
    "            for name_split_0 in name_splits:\n",
    "\n",
    "                df1 = splits[name_split][split_data]\n",
    "                df2 = splits[name_split_0][split_data]\n",
    "                df_merge = []\n",
    "\n",
    "                df_merge = pd.merge(df1, df2, on=['h', 'r', 't'], how='inner')\n",
    "                df1 = df1 + df_merge\n",
    "\n",
    "                overlap = np.sum(df1.duplicated(keep=False))  # keep=False marks the duplicated row with a True\n",
    "\n",
    "                overlapping[name_split][name_split_0] = (1-round(overlap / len(df1),4))*100\n",
    "\n",
    "        print(name + \"_\" + split_data + \" overlapping matrix:\")\n",
    "        print(overlapping.to_markdown())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ABLATION VALIDATION (CONTAINED INTO BASELINE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open FB15k_0_ComplEx_harmonic_centrality_0.05...\n",
      "Open FB15k_0_ComplEx_betweenness_-0.1...\n",
      "Open FB15k_0_ComplEx_harmonic_centrality_-0.15...\n",
      "Open FB15k_0_ComplEx_betweenness_-0.15...\n",
      "Open FB15k_0_ComplEx_harmonic_centrality_-0.2...\n",
      "Open FB15k_0_ComplEx_baseline_0...\n",
      "Open FB15k_0_ComplEx_betweenness_0.05...\n",
      "Open FB15k_0_ComplEx_harmonic_centrality_0.1...\n",
      "Open FB15k_0_ComplEx_betweenness_0.03...\n",
      "CHECK ABLATION FB15k_0_ComplEx_harmonic_centrality_0.05...\n",
      "CHECK ABLATION FB15k_0_ComplEx_betweenness_-0.1...\n",
      "CHECK ABLATION FB15k_0_ComplEx_harmonic_centrality_-0.15...\n",
      "CHECK ABLATION FB15k_0_ComplEx_betweenness_-0.15...\n",
      "CHECK ABLATION FB15k_0_ComplEx_harmonic_centrality_-0.2...\n",
      "CHECK ABLATION FB15k_0_ComplEx_betweenness_0.05...\n",
      "CHECK ABLATION FB15k_0_ComplEx_harmonic_centrality_0.1...\n",
      "CHECK ABLATION FB15k_0_ComplEx_betweenness_0.03...\n",
      "Open FB15k237_0_ComplEx_harmonic_centrality_-0.1...\n",
      "Open FB15k237_0_ComplEx_pagerank_0.03...\n",
      "Open FB15k237_0_ComplEx_pagerank_0.05...\n",
      "Open FB15k237_0_ComplEx_harmonic_centrality_0.01...\n",
      "Open FB15k237_0_ComplEx_pagerank_-0.15...\n",
      "Open FB15k237_0_ComplEx_baseline_0...\n",
      "Open FB15k237_0_ComplEx_pagerank_-0.1...\n",
      "Open FB15k237_0_ComplEx_harmonic_centrality_0.03...\n",
      "Open FB15k237_0_ComplEx_harmonic_centrality_-0.05...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_harmonic_centrality_-0.1...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_pagerank_0.03...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_pagerank_0.05...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_harmonic_centrality_0.01...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_pagerank_-0.15...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_pagerank_-0.1...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_harmonic_centrality_0.03...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_harmonic_centrality_-0.05...\n",
      "Open WN18_0_ComplEx_betweenness_0.03...\n",
      "Open WN18_0_ComplEx_baseline_0...\n",
      "Open WN18_0_ComplEx_betweenness_-0.2...\n",
      "Open WN18_0_ComplEx_pagerank_-0.05...\n",
      "Open WN18_0_ComplEx_pagerank_0.05...\n",
      "Open WN18_0_ComplEx_pagerank_-0.15...\n",
      "Open WN18_0_ComplEx_betweenness_0.1...\n",
      "Open WN18_0_ComplEx_betweenness_-0.1...\n",
      "Open WN18_0_ComplEx_pagerank_0.01...\n",
      "CHECK ABLATION WN18_0_ComplEx_betweenness_0.03...\n",
      "CHECK ABLATION WN18_0_ComplEx_betweenness_-0.2...\n",
      "CHECK ABLATION WN18_0_ComplEx_pagerank_-0.05...\n",
      "CHECK ABLATION WN18_0_ComplEx_pagerank_0.05...\n",
      "CHECK ABLATION WN18_0_ComplEx_pagerank_-0.15...\n",
      "CHECK ABLATION WN18_0_ComplEx_betweenness_0.1...\n",
      "CHECK ABLATION WN18_0_ComplEx_betweenness_-0.1...\n",
      "CHECK ABLATION WN18_0_ComplEx_pagerank_0.01...\n",
      "Open WN18RR_0_ComplEx_pagerank_0.05...\n",
      "Open WN18RR_0_ComplEx_pagerank_-0.2...\n",
      "Open WN18RR_0_ComplEx_degree_-0.1...\n",
      "Open WN18RR_0_ComplEx_pagerank_0.1...\n",
      "Open WN18RR_0_ComplEx_degree_0.1...\n",
      "Open WN18RR_0_ComplEx_pagerank_-0.15...\n",
      "Open WN18RR_0_ComplEx_degree_0.03...\n",
      "Open WN18RR_0_ComplEx_baseline_0...\n",
      "Open WN18RR_0_ComplEx_degree_-0.2...\n",
      "CHECK ABLATION WN18RR_0_ComplEx_pagerank_0.05...\n",
      "CHECK ABLATION WN18RR_0_ComplEx_pagerank_-0.2...\n",
      "CHECK ABLATION WN18RR_0_ComplEx_degree_-0.1...\n",
      "CHECK ABLATION WN18RR_0_ComplEx_pagerank_0.1...\n",
      "CHECK ABLATION WN18RR_0_ComplEx_degree_0.1...\n",
      "CHECK ABLATION WN18RR_0_ComplEx_pagerank_-0.15...\n",
      "CHECK ABLATION WN18RR_0_ComplEx_degree_0.03...\n",
      "CHECK ABLATION WN18RR_0_ComplEx_degree_-0.2...\n",
      "Open YAGO310_0_ComplEx_degree_0.1...\n",
      "Open YAGO310_0_ComplEx_degree_0.01...\n",
      "Open YAGO310_0_ComplEx_degree_-0.05...\n",
      "Open YAGO310_0_ComplEx_harmonic_centrality_-0.15...\n",
      "Open YAGO310_0_ComplEx_harmonic_centrality_0.05...\n",
      "Open YAGO310_0_ComplEx_baseline_0...\n",
      "Open YAGO310_0_ComplEx_harmonic_centrality_-0.05...\n",
      "Open YAGO310_0_ComplEx_harmonic_centrality_0.01...\n",
      "Open YAGO310_0_ComplEx_degree_-0.2...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_degree_0.1...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_degree_0.01...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_degree_-0.05...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_harmonic_centrality_-0.15...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_harmonic_centrality_0.05...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_harmonic_centrality_-0.05...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_harmonic_centrality_0.01...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_degree_-0.2...\n"
     ]
    }
   ],
   "source": [
    "directory = '../Training/TODO/'\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    ablations = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "\n",
    "        if name+\"_\" in filename:\n",
    "\n",
    "            print(\"Open \"+ filename+\"...\")\n",
    "\n",
    "            if \"baseline\" in filename:\n",
    "                with open(f+'/instance.pickle', 'rb') as f_load:\n",
    "                    pick = pickle.load(f_load)\n",
    "\n",
    "                    baseline = {\n",
    "                        'dataset': pick,\n",
    "                        #'training': pd.DataFrame(pick['training'], columns=['h', 'r', 't']),\n",
    "                        #'testing': pd.DataFrame(pick['testing'], columns=['h', 'r', 't']),\n",
    "                        #'validation': pd.DataFrame(pick['validation'], columns=['h', 'r', 't']),\n",
    "                        #'min_testing': pd.DataFrame(pick['min_testing'], columns=['h', 'r', 't'])\n",
    "\n",
    "                        'training': set([tuple(x) for x in pick['training']]),\n",
    "                        'testing': set([tuple(x) for x in pick['testing']]),\n",
    "                        'validation': set([tuple(x) for x in pick['validation']]),\n",
    "                        'min_testing': set([tuple(x) for x in pick['min_testing']]),\n",
    "\n",
    "                    }\n",
    "            else:\n",
    "                with open(f+'/instance.pickle', 'rb') as f_load:\n",
    "                    pick = pickle.load(f_load)\n",
    "\n",
    "                    abl = {\n",
    "                        'name': filename,\n",
    "                        'dataset': pick,\n",
    "                        'training': set([tuple(x) for x in pick['training']]),\n",
    "                        'testing': set([tuple(y) for y in pick['testing']]),\n",
    "                        'validation': set([tuple(z) for z in pick['validation']]),\n",
    "                        'min_testing': set([tuple(t) for t in pick['min_testing']])\n",
    "                    }\n",
    "\n",
    "                    ablations.append(abl)\n",
    "\n",
    "    training = baseline[\"training\"]\n",
    "    testing = baseline[\"testing\"]\n",
    "    validation = baseline[\"validation\"]\n",
    "    min_test = baseline[\"min_testing\"]\n",
    "\n",
    "    for a in ablations:\n",
    "\n",
    "        print(\"CHECK ABLATION \"+ a[\"name\"]+\"...\")\n",
    "        if str(a[\"training\"]-training) != \"set()\":\n",
    "            print(\"ERROR IN THE TRAINING ABLATION\")\n",
    "            print(a[\"training\"]-training)\n",
    "\n",
    "        if str(a[\"testing\"]-testing) != \"set()\":\n",
    "            print(\"ERROR IN THE TESTING ABLATION\")\n",
    "            print(a[\"testing\"]-testing)\n",
    "\n",
    "        if str(a[\"validation\"]-validation) != \"set()\":\n",
    "            print(\"ERROR IN THE VALIDATION ABLATION\")\n",
    "            print(a[\"validation\"]-validation)\n",
    "\n",
    "        if str(a[\"min_testing\"]-min_test) != \"set()\":\n",
    "            print(\"ERROR IN THE MIN TESTING ABLATION\")\n",
    "            print(a[\"min_testing\"]-min_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "OTHER CHECKS ON THE ABLATIONS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open FB15k_0_ComplEx_harmonic_centrality_0.05...\n",
      "Open FB15k_0_ComplEx_betweenness_-0.1...\n",
      "Open FB15k_0_ComplEx_harmonic_centrality_-0.15...\n",
      "Open FB15k_0_ComplEx_betweenness_-0.15...\n",
      "Open FB15k_0_ComplEx_harmonic_centrality_-0.2...\n",
      "Open FB15k_0_ComplEx_baseline_0...\n",
      "Open FB15k_0_ComplEx_betweenness_0.05...\n",
      "Open FB15k_0_ComplEx_harmonic_centrality_0.1...\n",
      "Open FB15k_0_ComplEx_betweenness_0.03...\n",
      "CHECK QUALITY BASELINE\n",
      "CHECK QUALITY ABLATIONS\n",
      "CHECK ABLATION FB15k_0_ComplEx_harmonic_centrality_0.05...\n",
      "CHECK ABLATION FB15k_0_ComplEx_betweenness_-0.1...\n",
      "CHECK ABLATION FB15k_0_ComplEx_harmonic_centrality_-0.15...\n",
      "CHECK ABLATION FB15k_0_ComplEx_betweenness_-0.15...\n",
      "CHECK ABLATION FB15k_0_ComplEx_harmonic_centrality_-0.2...\n",
      "CHECK ABLATION FB15k_0_ComplEx_betweenness_0.05...\n",
      "CHECK ABLATION FB15k_0_ComplEx_harmonic_centrality_0.1...\n",
      "CHECK ABLATION FB15k_0_ComplEx_betweenness_0.03...\n",
      "Open FB15k237_0_ComplEx_harmonic_centrality_-0.1...\n",
      "Open FB15k237_0_ComplEx_pagerank_0.03...\n",
      "Open FB15k237_0_ComplEx_pagerank_0.05...\n",
      "Open FB15k237_0_ComplEx_harmonic_centrality_0.01...\n",
      "Open FB15k237_0_ComplEx_pagerank_-0.15...\n",
      "Open FB15k237_0_ComplEx_baseline_0...\n",
      "Open FB15k237_0_ComplEx_pagerank_-0.1...\n",
      "Open FB15k237_0_ComplEx_harmonic_centrality_0.03...\n",
      "Open FB15k237_0_ComplEx_harmonic_centrality_-0.05...\n",
      "CHECK QUALITY BASELINE\n",
      "CHECK QUALITY ABLATIONS\n",
      "CHECK ABLATION FB15k237_0_ComplEx_harmonic_centrality_-0.1...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_pagerank_0.03...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_pagerank_0.05...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_harmonic_centrality_0.01...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_pagerank_-0.15...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_pagerank_-0.1...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_harmonic_centrality_0.03...\n",
      "CHECK ABLATION FB15k237_0_ComplEx_harmonic_centrality_-0.05...\n",
      "Open WN18_0_ComplEx_betweenness_0.03...\n",
      "Open WN18_0_ComplEx_baseline_0...\n",
      "Open WN18_0_ComplEx_betweenness_-0.2...\n",
      "Open WN18_0_ComplEx_pagerank_-0.05...\n",
      "Open WN18_0_ComplEx_pagerank_0.05...\n",
      "Open WN18_0_ComplEx_pagerank_-0.15...\n",
      "Open WN18_0_ComplEx_betweenness_0.1...\n",
      "Open WN18_0_ComplEx_betweenness_-0.1...\n",
      "Open WN18_0_ComplEx_pagerank_0.01...\n",
      "CHECK QUALITY BASELINE\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK QUALITY ABLATIONS\n",
      "CHECK ABLATION WN18_0_ComplEx_betweenness_0.03...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18_0_ComplEx_betweenness_-0.2...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18_0_ComplEx_pagerank_-0.05...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18_0_ComplEx_pagerank_0.05...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18_0_ComplEx_pagerank_-0.15...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18_0_ComplEx_betweenness_0.1...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18_0_ComplEx_betweenness_-0.1...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18_0_ComplEx_pagerank_0.01...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open WN18RR_0_ComplEx_pagerank_0.05...\n",
      "Open WN18RR_0_ComplEx_pagerank_-0.2...\n",
      "Open WN18RR_0_ComplEx_degree_-0.1...\n",
      "Open WN18RR_0_ComplEx_pagerank_0.1...\n",
      "Open WN18RR_0_ComplEx_degree_0.1...\n",
      "Open WN18RR_0_ComplEx_pagerank_-0.15...\n",
      "Open WN18RR_0_ComplEx_degree_0.03...\n",
      "Open WN18RR_0_ComplEx_baseline_0...\n",
      "Open WN18RR_0_ComplEx_degree_-0.2...\n",
      "CHECK QUALITY BASELINE\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK QUALITY ABLATIONS\n",
      "CHECK ABLATION WN18RR_0_ComplEx_pagerank_0.05...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18RR_0_ComplEx_pagerank_-0.2...\n",
      "CHECK ABLATION WN18RR_0_ComplEx_degree_-0.1...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18RR_0_ComplEx_pagerank_0.1...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18RR_0_ComplEx_degree_0.1...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18RR_0_ComplEx_pagerank_-0.15...\n",
      "CHECK ABLATION WN18RR_0_ComplEx_degree_0.03...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "CHECK ABLATION WN18RR_0_ComplEx_degree_-0.2...\n",
      "('h', 't') have high correlation\n",
      "('t', 'h') have high correlation\n",
      "Open YAGO310_0_ComplEx_degree_0.1...\n",
      "Open YAGO310_0_ComplEx_degree_0.01...\n",
      "Open YAGO310_0_ComplEx_degree_-0.05...\n",
      "Open YAGO310_0_ComplEx_harmonic_centrality_-0.15...\n",
      "Open YAGO310_0_ComplEx_harmonic_centrality_0.05...\n",
      "Open YAGO310_0_ComplEx_baseline_0...\n",
      "Open YAGO310_0_ComplEx_harmonic_centrality_-0.05...\n",
      "Open YAGO310_0_ComplEx_harmonic_centrality_0.01...\n",
      "Open YAGO310_0_ComplEx_degree_-0.2...\n",
      "CHECK QUALITY BASELINE\n",
      "CHECK QUALITY ABLATIONS\n",
      "CHECK ABLATION YAGO310_0_ComplEx_degree_0.1...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_degree_0.01...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_degree_-0.05...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_harmonic_centrality_-0.15...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_harmonic_centrality_0.05...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_harmonic_centrality_-0.05...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_harmonic_centrality_0.01...\n",
      "CHECK ABLATION YAGO310_0_ComplEx_degree_-0.2...\n"
     ]
    }
   ],
   "source": [
    "directory = '../Training/TODO/'\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    ablations = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "\n",
    "        if name+\"_\" in filename:\n",
    "\n",
    "            print(\"Open \"+ filename+\"...\")\n",
    "\n",
    "            if \"baseline\" in filename:\n",
    "                with open(f+'/instance.pickle', 'rb') as f_load:\n",
    "                    pick = pickle.load(f_load)\n",
    "\n",
    "                    baseline = {\n",
    "                        'dataset': pick,\n",
    "                        'training': pd.DataFrame(pick['training'], columns=['h', 'r', 't']),\n",
    "                        'testing': pd.DataFrame(pick['testing'], columns=['h', 'r', 't']),\n",
    "                        'validation': pd.DataFrame(pick['validation'], columns=['h', 'r', 't']),\n",
    "                        'min_testing': pd.DataFrame(pick['min_testing'], columns=['h', 'r', 't'])\n",
    "                    }\n",
    "            else:\n",
    "                with open(f+'/instance.pickle', 'rb') as f_load:\n",
    "                    pick = pickle.load(f_load)\n",
    "\n",
    "                    abl = {\n",
    "                        'name': filename,\n",
    "                        'dataset': pick,\n",
    "                        'training': pd.DataFrame(pick['training'], columns=['h', 'r', 't']),\n",
    "                        'testing': pd.DataFrame(pick['testing'], columns=['h', 'r', 't']),\n",
    "                        'validation': pd.DataFrame(pick['validation'], columns=['h', 'r', 't']),\n",
    "                        'min_testing': pd.DataFrame(pick['min_testing'], columns=['h', 'r', 't'])\n",
    "                    }\n",
    "\n",
    "                    ablations.append(abl)\n",
    "\n",
    "    training = baseline[\"training\"]\n",
    "    testing = baseline[\"testing\"]\n",
    "    validation = baseline[\"validation\"]\n",
    "    min_test = baseline[\"min_testing\"]\n",
    "\n",
    "    print(\"CHECK QUALITY BASELINE\")\n",
    "    check_quality(training)\n",
    "    check_quality(testing)\n",
    "    check_quality(validation)\n",
    "    check_quality(min_test)\n",
    "    check_split(training,testing,validation)\n",
    "\n",
    "    print(\"CHECK QUALITY ABLATIONS\")\n",
    "    for a in ablations:\n",
    "        print(\"CHECK ABLATION \"+ a[\"name\"]+\"...\")\n",
    "        check_quality(a[\"training\"])\n",
    "        check_quality(a[\"testing\"])\n",
    "        check_quality(a[\"validation\"])\n",
    "        check_quality(a[\"min_testing\"])\n",
    "        check_split(a[\"training\"],a[\"testing\"],a[\"validation\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
